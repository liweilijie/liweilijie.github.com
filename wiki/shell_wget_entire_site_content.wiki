%toc wget下载整个网站
%title wget下载整个网站


----

`wget -r -p -k -np site_url`

_由于有时候需要涉及到一些web相关的东西，要用别人网站的html做参考，所以干脆将此网站里的所有url download下来慢慢分析，所以就有了今天这页博文。_

----

wget下载整个网站可以使用下面的命令 `wget -r -p -k -np http://www.after90.org/` . 

* `-r` 表示递归下载,会下载所有的链接,不过要注意的是,不要单独使用这个参数,因为如果你要下载的网站也有别的网站的链接, *wget* 也会把别的网站的东西下载下来,由于互联网的特性,很有可能你会把整个互联网给下载下来.
* `-np` 由于上面的原因，所以要加上 `-np` 这个参数, 表示不下载别的站点的链接. 
* `-k` 表示将下载的网页里的链接修改为本地链接.
* `-p` 获得所以显示网页所需的元素,比如图片什么的.

----

*另外还有其他的一些参数可以使用:*
 
* `-c` 表示断点续传
* `-t 100` 表示重试100次, `-t 0` 表示无穷次重试
* `-i filename` 另外可以将要下载的 *url* 写到一个文件中,每个 *url* 一行,使用这样的命令 `wget -i download.txt`.
* `--reject=avi,rmvb` 表示不下载 *avi* , *rmvb* 的文件,
* `--accept=jpg,jpeg`,表示只下载 *jpg* , *jpeg* 的文件.

可以在用户目录下建立一个 *.wgetrc* 的文件(windows里面好像不能直接建立这样的文件,windows会认为没有文件名--),里面写上 `http-proxy = 123.456.78.9:80`, 然后在加上参数 `--proxy=on`, 如果需要密码,再加上下面的参数 `--proxy-user=username`, `--proxy-passwd=password`.
